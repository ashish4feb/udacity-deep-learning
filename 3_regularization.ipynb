{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Assignment 3\n",
    "\n",
    "Previously in 2_fullyconnected.ipynb, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First reload the data we generated in 1_notmnist.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor t using nn.l2_loss(t). The right amount of regularization should improve your validation / test accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape = (batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32,\n",
    "                                    shape = (batch_size,num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #Parameters\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #Computation\n",
    "    logits = tf.matmul(tf_train_dataset,weights)+biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels = tf_train_labels,logits= logits) + beta*tf.nn.l2_loss(weights))\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 22.087166\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 15.9%\n",
      "Minibatch loss at step 500: 2.554112\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 1000: 1.725648\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1500: 0.976111\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 2000: 0.866525\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2500: 0.864004\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 3000: 0.755946\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 88.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step*batch_size)%(train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_label = train_labels[offset:(offset+batch_size)]\n",
    "        feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_label}\n",
    "        _,l,prediction = session.run([optimizer,loss,train_prediction],feed_dict = feed_dict)\n",
    "        if (step%500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(prediction, batch_label))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                    valid_prediction.eval(),valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #constants\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=[batch_size,image_size*image_size])\n",
    "    tf_train_labels = tf.placeholder(tf.float32,shape=[batch_size,num_labels])\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    #variables\n",
    "    weight_1 = tf.Variable(tf.truncated_normal([image_size*image_size,1024]))\n",
    "    biases_1 = tf.Variable(tf.zeros([1024]))\n",
    "    \n",
    "    weight_2 = tf.Variable(tf.truncated_normal([1024,num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #Computation\n",
    "    hidden = tf.nn.relu(tf.matmul(tf_train_dataset,weight_1)+biases_1)\n",
    "    \n",
    "    logits = tf.matmul(hidden,weight_2)+biases_2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf_train_labels,logits=logits)+beta*tf.nn.l2_loss(weight_2) + beta*tf.nn.l2_loss(weight_1))\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #prediction\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset,weight_1)+biases_1),weight_2)+biases_2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset,weight_1)+biases_1),weight_2)+biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "Minibatch loss at step 0: 645.293640\n",
      "Train accuracy 14.1%\n",
      "Valid accuracy 33.1%\n",
      "Minibatch loss at step 500: 199.374329\n",
      "Train accuracy 81.2%\n",
      "Valid accuracy 78.9%\n",
      "Minibatch loss at step 1000: 115.399429\n",
      "Train accuracy 81.2%\n",
      "Valid accuracy 81.4%\n",
      "Minibatch loss at step 1500: 68.824875\n",
      "Train accuracy 91.4%\n",
      "Valid accuracy 83.6%\n",
      "Minibatch loss at step 2000: 41.233017\n",
      "Train accuracy 92.2%\n",
      "Valid accuracy 84.7%\n",
      "Minibatch loss at step 2500: 25.168875\n",
      "Train accuracy 89.8%\n",
      "Valid accuracy 85.5%\n",
      "Minibatch loss at step 3000: 15.483320\n",
      "Train accuracy 84.4%\n",
      "Valid accuracy 86.4%\n",
      "Test accuracy 93.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    print(\"initialized\")\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = batch_size*step%(train_dataset.shape[0]-batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size),:]\n",
    "        feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_labels}\n",
    "        _,l,predictions = sess.run([optimizer,loss,train_prediction],feed_dict=feed_dict)\n",
    "        if step%500==0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Train accuracy %.1f%%\" % accuracy(predictions,batch_labels))\n",
    "            print(\"Valid accuracy %.1f%%\" % accuracy(valid_prediction.eval(),valid_labels))\n",
    "    print(\"Test accuracy %.1f%%\" % accuracy(test_prediction.eval(),test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #constants\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=[batch_size,image_size*image_size])\n",
    "    tf_train_labels = tf.placeholder(tf.float32,shape=[batch_size,num_labels])\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    #variables\n",
    "    weight_1 = tf.Variable(tf.truncated_normal([image_size*image_size,1024]))\n",
    "    biases_1 = tf.Variable(tf.zeros([1024]))\n",
    "    \n",
    "    weight_2 = tf.Variable(tf.truncated_normal([1024,num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #Computation\n",
    "    hidden = tf.nn.relu(tf.matmul(tf_train_dataset,weight_1)+biases_1)\n",
    "    \n",
    "    logits = tf.matmul(hidden,weight_2)+biases_2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf_train_labels,logits=logits)+beta*tf.nn.l2_loss(weight_2) + beta*tf.nn.l2_loss(weight_1))\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #prediction\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset,weight_1)+biases_1),weight_2)+biases_2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset,weight_1)+biases_1),weight_2)+biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "Minibatch loss at step 0: 633.627808\n",
      "Train accuracy 15.6%\n",
      "Valid accuracy 27.8%\n",
      "Minibatch loss at step 500: 190.189850\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 75.4%\n",
      "Minibatch loss at step 1000: 115.341507\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 75.4%\n",
      "Minibatch loss at step 1500: 69.949417\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 75.4%\n",
      "Minibatch loss at step 2000: 42.421204\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 75.2%\n",
      "Minibatch loss at step 2500: 25.726685\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 75.2%\n",
      "Minibatch loss at step 3000: 15.602524\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 75.4%\n",
      "Test accuracy 82.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "sam_train_data = train_dataset[:500,:]\n",
    "sam_train_labels = train_labels[:500,:]\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    print(\"initialized\")\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = batch_size*step%(sam_train_data.shape[0]-batch_size)\n",
    "        batch_data = sam_train_data[offset:(offset+batch_size),:]\n",
    "        batch_labels = sam_train_labels[offset:(offset+batch_size),:]\n",
    "        feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_labels}\n",
    "        _,l,predictions = sess.run([optimizer,loss,train_prediction],feed_dict=feed_dict)\n",
    "        if step%500==0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Train accuracy %.1f%%\" % accuracy(predictions,batch_labels))\n",
    "            print(\"Valid accuracy %.1f%%\" % accuracy(valid_prediction.eval(),valid_labels))\n",
    "    print(\"Test accuracy %.1f%%\" % accuracy(test_prediction.eval(),test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training.\n",
    "What happens to our extreme overfitting case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #constants\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=[batch_size,image_size*image_size])\n",
    "    tf_train_labels = tf.placeholder(tf.float32,shape=[batch_size,num_labels])\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_keep_prob = tf.constant(0.5)\n",
    "    #variables\n",
    "    weight_1 = tf.Variable(tf.truncated_normal([image_size*image_size,1024]))\n",
    "    biases_1 = tf.Variable(tf.zeros([1024]))\n",
    "    \n",
    "    weight_2 = tf.Variable(tf.truncated_normal([1024,num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #Computation\n",
    "    hidden = tf.nn.relu(tf.matmul(tf_train_dataset,weight_1)+biases_1)\n",
    "    drop_hidden = tf.nn.dropout(hidden,keep_prob=tf_keep_prob)\n",
    "    \n",
    "    logits = tf.matmul(drop_hidden,weight_2)+biases_2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf_train_labels,logits=logits)+beta*tf.nn.l2_loss(weight_2) + beta*tf.nn.l2_loss(weight_1))\n",
    "    \n",
    "    #optimizer\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.001, global_step, 3000,0.96,staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(0.03).minimize(loss)\n",
    "    \n",
    "    \n",
    "    #prediction\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset,weight_1)+biases_1),weight_2)+biases_2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset,weight_1)+biases_1),weight_2)+biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "Minibatch loss at step 0: 4144.485352\n",
      "Train accuracy 10.2%\n",
      "Valid accuracy 11.7%\n",
      "Minibatch loss at step 500: 714.283081\n",
      "Train accuracy 69.5%\n",
      "Valid accuracy 71.1%\n",
      "Minibatch loss at step 1000: 723.118103\n",
      "Train accuracy 74.2%\n",
      "Valid accuracy 73.6%\n",
      "Minibatch loss at step 1500: 570.418579\n",
      "Train accuracy 80.5%\n",
      "Valid accuracy 74.8%\n",
      "Minibatch loss at step 2000: 602.315491\n",
      "Train accuracy 77.3%\n",
      "Valid accuracy 75.7%\n",
      "Minibatch loss at step 2500: 572.526733\n",
      "Train accuracy 75.8%\n",
      "Valid accuracy 76.0%\n",
      "Minibatch loss at step 3000: 561.943848\n",
      "Train accuracy 72.7%\n",
      "Valid accuracy 76.5%\n",
      "Test accuracy 84.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    print(\"initialized\")\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = batch_size*step%(train_dataset.shape[0]-batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size),:]\n",
    "        feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_labels}\n",
    "        _,l,predictions = sess.run([optimizer,loss,train_prediction],feed_dict=feed_dict)\n",
    "        if step%500==0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Train accuracy %.1f%%\" % accuracy(predictions,batch_labels))\n",
    "            print(\"Valid accuracy %.1f%%\" % accuracy(valid_prediction.eval(),valid_labels))\n",
    "        #global_step+=1\n",
    "    print(\"Test accuracy %.1f%%\" % accuracy(test_prediction.eval(),test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limited Dataset Solution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "Minibatch loss at step 0: 768.750000\n",
      "Train accuracy 9.4%\n",
      "Valid accuracy 32.3%\n",
      "Minibatch loss at step 500: 191.571442\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 78.4%\n",
      "Minibatch loss at step 1000: 116.235710\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 78.5%\n",
      "Minibatch loss at step 1500: 70.520401\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 78.2%\n",
      "Minibatch loss at step 2000: 42.773659\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 78.7%\n",
      "Minibatch loss at step 2500: 25.942890\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 78.4%\n",
      "Minibatch loss at step 3000: 15.733993\n",
      "Train accuracy 100.0%\n",
      "Valid accuracy 78.5%\n",
      "Test accuracy 85.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "sam_train_data = train_dataset[:500,:]\n",
    "sam_train_labels = train_labels[:500,:]\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    print(\"initialized\")\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = batch_size*step%(sam_train_data.shape[0]-batch_size)\n",
    "        batch_data = sam_train_data[offset:(offset+batch_size),:]\n",
    "        batch_labels = sam_train_labels[offset:(offset+batch_size),:]\n",
    "        feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_labels}\n",
    "        _,l,predictions = sess.run([optimizer,loss,train_prediction],feed_dict=feed_dict)\n",
    "        if step%500==0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Train accuracy %.1f%%\" % accuracy(predictions,batch_labels))\n",
    "            print(\"Valid accuracy %.1f%%\" % accuracy(valid_prediction.eval(),valid_labels))\n",
    "    print(\"Test accuracy %.1f%%\" % accuracy(test_prediction.eval(),test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1%.\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #constants\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=[batch_size,image_size*image_size])\n",
    "    tf_train_labels = tf.placeholder(tf.float32,shape=[batch_size,num_labels])\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_keep_prob = tf.constant(1.0)\n",
    "    #variables\n",
    "    weight_1 = tf.Variable(tf.truncated_normal([image_size*image_size,1024]))\n",
    "    biases_1 = tf.Variable(tf.zeros([1024]))\n",
    "    \n",
    "    weight_2 = tf.Variable(tf.truncated_normal([1024,256]))\n",
    "    biases_2 = tf.Variable(tf.zeros([256]))\n",
    "    \n",
    "    #weight_3 = tf.Variable(tf.truncated_normal([256,128]))\n",
    "    #biases_3 = tf.Variable(tf.zeros([128]))\n",
    "    \n",
    "    weight_final = tf.Variable(tf.truncated_normal([256,num_labels]))\n",
    "    biases_final = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #Computation\n",
    "    \n",
    "    hidden_1 = tf.nn.relu(tf.matmul(tf_train_dataset,weight_1)+biases_1)\n",
    "    #drop_hidden_1 = tf.nn.dropout(hidden_1,keep_prob=tf_keep_prob)\n",
    "\n",
    "    hidden_2 = tf.nn.relu(tf.matmul(hidden_1,weight_2)+biases_2)\n",
    "    #drop_hidden_2 = tf.nn.dropout(hidden_2,keep_prob=tf_keep_prob)\n",
    "\n",
    "    #hidden_3 = tf.nn.relu(tf.matmul(drop_hidden_2,weight_3)+biases_3)\n",
    "    #drop_hidden_3 = tf.nn.dropout(hidden_3,keep_prob=tf_keep_prob)\n",
    "    \n",
    "    logits = tf.matmul(hidden_2,weight_final)+biases_final\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf_train_labels,logits=logits)+\n",
    "                          beta*tf.nn.l2_loss(weight_final) +\n",
    "                         beta*tf.nn.l2_loss(weight_1)  +\n",
    "                         beta*tf.nn.l2_loss(weight_2) \n",
    "                         #beta*tf.nn.l2_loss(weight_3)\n",
    "                         )\n",
    "    \n",
    "    #optimizer\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.001, global_step, 1000,0.90,staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    #prediction\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    #Validation\n",
    "    hidden_val_1 = tf.nn.relu(tf.matmul(tf_valid_dataset,weight_1)+biases_1)\n",
    "    hidden_val_2 = tf.nn.relu(tf.matmul(hidden_val_1,weight_2)+biases_2)\n",
    "    #hidden_val_3 = tf.nn.relu(tf.matmul(hidden_val_2,weight_3)+biases_3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_val_2,weight_final)+biases_final)\n",
    "    \n",
    "    #Test\n",
    "    hidden_test_1 = tf.nn.relu(tf.matmul(tf_test_dataset,weight_1)+biases_1)\n",
    "    hidden_test_2 = tf.nn.relu(tf.matmul(hidden_test_1,weight_2)+biases_2)\n",
    "    #hidden_test_3 = tf.nn.relu(tf.matmul(hidden_test_2,weight_3)+biases_3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test_2,weight_final)+biases_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "Minibatch loss at step 0: 3315.189209\n",
      "Train accuracy 14.1%\n",
      "Valid accuracy 14.1%\n",
      "Minibatch loss at step 500: 639.918518\n",
      "Train accuracy 80.5%\n",
      "Valid accuracy 75.0%\n",
      "Minibatch loss at step 1000: 669.706482\n",
      "Train accuracy 75.0%\n",
      "Valid accuracy 76.0%\n",
      "Minibatch loss at step 1500: 536.940186\n",
      "Train accuracy 80.5%\n",
      "Valid accuracy 77.0%\n",
      "Minibatch loss at step 2000: 541.268677\n",
      "Train accuracy 78.9%\n",
      "Valid accuracy 77.5%\n",
      "Minibatch loss at step 2500: 502.972504\n",
      "Train accuracy 79.7%\n",
      "Valid accuracy 77.5%\n",
      "Minibatch loss at step 3000: 481.505310\n",
      "Train accuracy 75.8%\n",
      "Valid accuracy 77.4%\n",
      "Minibatch loss at step 3500: 485.201111\n",
      "Train accuracy 77.3%\n",
      "Valid accuracy 77.5%\n",
      "Minibatch loss at step 4000: 456.499939\n",
      "Train accuracy 78.9%\n",
      "Valid accuracy 77.1%\n",
      "Minibatch loss at step 4500: 447.071747\n",
      "Train accuracy 79.7%\n",
      "Valid accuracy 77.5%\n",
      "Minibatch loss at step 5000: 447.720215\n",
      "Train accuracy 82.0%\n",
      "Valid accuracy 77.5%\n",
      "Minibatch loss at step 5500: 446.184814\n",
      "Train accuracy 74.2%\n",
      "Valid accuracy 77.0%\n",
      "Minibatch loss at step 6000: 444.829346\n",
      "Train accuracy 74.2%\n",
      "Valid accuracy 77.2%\n",
      "Minibatch loss at step 6500: 435.342224\n",
      "Train accuracy 81.2%\n",
      "Valid accuracy 77.0%\n",
      "Minibatch loss at step 7000: 432.557312\n",
      "Train accuracy 75.0%\n",
      "Valid accuracy 77.3%\n",
      "Minibatch loss at step 7500: 449.882996\n",
      "Train accuracy 72.7%\n",
      "Valid accuracy 77.0%\n",
      "Minibatch loss at step 8000: 440.563904\n",
      "Train accuracy 75.0%\n",
      "Valid accuracy 75.5%\n",
      "Minibatch loss at step 8500: 430.114349\n",
      "Train accuracy 77.3%\n",
      "Valid accuracy 77.0%\n",
      "Minibatch loss at step 9000: 439.401184\n",
      "Train accuracy 69.5%\n",
      "Valid accuracy 76.1%\n",
      "Test accuracy 83.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    print(\"initialized\")\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = batch_size*step%(train_dataset.shape[0]-batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size),:]\n",
    "        feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_labels}\n",
    "        _,l,predictions = sess.run([optimizer,loss,train_prediction],feed_dict=feed_dict)\n",
    "        if step%500==0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Train accuracy %.1f%%\" % accuracy(predictions,batch_labels))\n",
    "            print(\"Valid accuracy %.1f%%\" % accuracy(valid_prediction.eval(),valid_labels))\n",
    "        #global_step+=1\n",
    "    print(\"Test accuracy %.1f%%\" % accuracy(test_prediction.eval(),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
